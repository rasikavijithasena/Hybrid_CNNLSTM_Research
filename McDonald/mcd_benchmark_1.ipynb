{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load local .csv file as DataFrame\n",
    "df = pd.read_csv('mcd_merge.csv')\n",
    "# Inspect the data\n",
    "\n",
    "df['Date'] = df['Datetime'].str[:-6]\n",
    "df['Date']=pd.to_datetime(df[\"Date\"]).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "data_date = df.filter(['Date'])\n",
    "\n",
    "data_date = data_date.values\n",
    "# Get the number of rows to train the model on\n",
    "training_data_date = int(np.ceil( len(data_date) * .95 ))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_data_date = data_date[0:int(training_data_date), :]\n",
    "\n",
    "# Create a new dataframe with only the 'Close column \n",
    "data = df.filter(['Close'])\n",
    "# Convert the dataframe to a numpy array\n",
    "dataset = data.values\n",
    "# Get the number of rows to train the model on\n",
    "training_data_len = int(np.ceil( len(dataset) * .95 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.56458207, 0.5641259 , 0.57197607, 0.57987219, 0.57827458,\n",
      "       0.57873074, 0.57416703, 0.57804615, 0.57599238, 0.57485163,\n",
      "       0.57188483, 0.56823345, 0.56663584, 0.55682314, 0.55408477,\n",
      "       0.55545395, 0.54815119, 0.54130528, 0.53263333, 0.5248744 ,\n",
      "       0.51939767, 0.52441824, 0.53035113, 0.53217717, 0.52898194,\n",
      "       0.52122302, 0.52692817, 0.53308949, 0.54404364, 0.54187565,\n",
      "       0.53354635, 0.55134641, 0.56047452, 0.57325401, 0.56640811,\n",
      "       0.57827458, 0.56366905, 0.5595615 , 0.5618437 , 0.56047452,\n",
      "       0.56672707, 0.56047452, 0.56321288, 0.56047452, 0.56093068,\n",
      "       0.56001836, 0.56275672, 0.56229986, 0.56376097, 0.55819232,\n",
      "       0.54952037, 0.55317175, 0.56344131, 0.56458207, 0.56229986,\n",
      "       0.55317175, 0.55088955, 0.55864918, 0.55499779, 0.56823345])]\n",
      "[0.5750793582256062]\n",
      "\n",
      "[array([0.56458207, 0.5641259 , 0.57197607, 0.57987219, 0.57827458,\n",
      "       0.57873074, 0.57416703, 0.57804615, 0.57599238, 0.57485163,\n",
      "       0.57188483, 0.56823345, 0.56663584, 0.55682314, 0.55408477,\n",
      "       0.55545395, 0.54815119, 0.54130528, 0.53263333, 0.5248744 ,\n",
      "       0.51939767, 0.52441824, 0.53035113, 0.53217717, 0.52898194,\n",
      "       0.52122302, 0.52692817, 0.53308949, 0.54404364, 0.54187565,\n",
      "       0.53354635, 0.55134641, 0.56047452, 0.57325401, 0.56640811,\n",
      "       0.57827458, 0.56366905, 0.5595615 , 0.5618437 , 0.56047452,\n",
      "       0.56672707, 0.56047452, 0.56321288, 0.56047452, 0.56093068,\n",
      "       0.56001836, 0.56275672, 0.56229986, 0.56376097, 0.55819232,\n",
      "       0.54952037, 0.55317175, 0.56344131, 0.56458207, 0.56229986,\n",
      "       0.55317175, 0.55088955, 0.55864918, 0.55499779, 0.56823345]), array([0.5641259 , 0.57197607, 0.57987219, 0.57827458, 0.57873074,\n",
      "       0.57416703, 0.57804615, 0.57599238, 0.57485163, 0.57188483,\n",
      "       0.56823345, 0.56663584, 0.55682314, 0.55408477, 0.55545395,\n",
      "       0.54815119, 0.54130528, 0.53263333, 0.5248744 , 0.51939767,\n",
      "       0.52441824, 0.53035113, 0.53217717, 0.52898194, 0.52122302,\n",
      "       0.52692817, 0.53308949, 0.54404364, 0.54187565, 0.53354635,\n",
      "       0.55134641, 0.56047452, 0.57325401, 0.56640811, 0.57827458,\n",
      "       0.56366905, 0.5595615 , 0.5618437 , 0.56047452, 0.56672707,\n",
      "       0.56047452, 0.56321288, 0.56047452, 0.56093068, 0.56001836,\n",
      "       0.56275672, 0.56229986, 0.56376097, 0.55819232, 0.54952037,\n",
      "       0.55317175, 0.56344131, 0.56458207, 0.56229986, 0.55317175,\n",
      "       0.55088955, 0.55864918, 0.55499779, 0.56823345, 0.57507936])]\n",
      "[0.5750793582256062, 0.5654950852918095]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "# Create the training data set \n",
    "# Create the scaled training data set\n",
    "train_data = scaled_data[0:int(training_data_len), :]\n",
    "# Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "    if i<= 61:\n",
    "        print(x_train)\n",
    "        print(y_train)\n",
    "        print()\n",
    "        \n",
    "# Convert the x_train and y_train to numpy arrays \n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Reshape the data\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "413/413 [==============================] - 4s 6ms/step - loss: 0.0022\n",
      "Epoch 2/8\n",
      "413/413 [==============================] - 4s 9ms/step - loss: 5.5634e-04\n",
      "Epoch 3/8\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 5.0943e-04\n",
      "Epoch 4/8\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 3.5252e-04\n",
      "Epoch 5/8\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 3.5895e-04\n",
      "Epoch 6/8\n",
      "413/413 [==============================] - 2s 5ms/step - loss: 3.5277e-04\n",
      "Epoch 7/8\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 3.1429e-04\n",
      "Epoch 8/8\n",
      "413/413 [==============================] - 2s 4ms/step - loss: 2.4652e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x147667ab0d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape= (x_train.shape[1], 1)))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dense(1))\n",
    "model2.compile(optimizer='adam', loss='mse')\n",
    "# Train the model\n",
    "model2.fit(x_train, y_train, batch_size=16, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4452703960326856"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = scaled_data[training_data_len - 60: , :]\n",
    "# Create the data sets x_test and y_test\n",
    "x_test = []\n",
    "y_test = dataset[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i-60:i, 0])\n",
    "    \n",
    "# Convert the data to a numpy array\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Reshape the data\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "# Get the models predicted price values \n",
    "predictions2 = model2.predict(x_test)\n",
    "predictions2 = scaler.inverse_transform(predictions2)\n",
    "\n",
    "# Get the root mean squared error (RMSE)\n",
    "rmse_2 = np.sqrt(np.mean(((predictions2 - y_test) ** 2)))\n",
    "rmse_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.mean(np.abs((actual - pred) / actual)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-f46f1df8d4ee>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_2['Predictions2'] = predictions2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1656082041396672"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data[:training_data_len]\n",
    "valid_2 = data[training_data_len:]\n",
    "valid_2['Predictions2'] = predictions2\n",
    "\n",
    "mape(valid_2['Close'], valid_2['Predictions2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
